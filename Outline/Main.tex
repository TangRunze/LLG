\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}

\usepackage{theorem}
\usepackage{amssymb}

\usepackage{hyperref}

\newenvironment{proof}{{\bf Proof:  }}{\hfill\rule{2mm}{2mm}}

\newtheorem{fact}{Fact}[section]
\newtheorem{lemma}[fact]{Lemma}
\newtheorem{theorem}[fact]{Theorem}
\newtheorem{definition}[fact]{Definition}
\newtheorem{corollary}[fact]{Corollary}
\newtheorem{proposition}[fact]{Proposition}
\newtheorem{claim}[fact]{Claim}
\newtheorem{exercise}[fact]{Exercise}

\newcommand{\RT}[1]{\marginpar{\footnotesize\color{red}RT: #1}}

\title{Outline for LLG}

\date{\today}

\begin{document}
\maketitle

\section{Introduction}

\begin{itemize}
\item Estimating the mean of a collection of graphs is becoming more and more important both in statistical inference and in various applications like connectomics, social networks, etc.
\item Element-wise maximum likelihood estimate is a reasonable estimator if we only consider the independent graph model without taking any graph structure into account.
\item However, in a large graph, vertices are generally clustered into different communities such that vertices of the same community behave similarly. The stochastic blockmodel (SBM) captures such structural property and is widely used in modeling networks.
\item Also, latent positions graph model proposes a way to parameterize the graph structure by latent positions associated with each vertex. And random dot product graph, a special case of latent positions graph, is considered in this paper. In particular, this paper considers SBM as a RDPG. So we will have exactly K different latent positions for N vertices.
\item Using the estimates of the latent positions in an RDPG based on a truncated eigen-decomposition of the adjacency matrix, we invent a new estimator for the mean of the collection of graphs which captures the low-rank structure. And we prove theoretically that it is better than element-wise MLE.
\end{itemize}


\section{Model}

\begin{itemize}
\item The goal is to estimate the mean of a collection of unweighted simple graphs by observing their adjacency matrices with known vertex correspondence.
\item This work considers the scenario of having $M$ graphs represented as adjacency matrices, each having $N$ vertices with known correspondence. And much more detailed description.
\end{itemize}

\subsection{Entry-Wise Least Squares Estimate}
\begin{itemize}
\item The most intuitive approach in this scenario is the element-wise mean among the adjacency matrices.
\end{itemize}

\subsection{Random Dot Product Graph}
\begin{itemize}
\item Hoff et. al. (2002) proposed a model for random graphs called Latent Positions Graph Model.
\item A specific instance of this model that we will examine is the random dot product graph model (RDPG) in which the link function is the dot product, i.e. the probability of an edge being present between two nodes is the dot product of their latent vectors.
\end{itemize}

\subsection{Stochastic Block Model as a Random Dot Product Graph}
\begin{itemize}
\item Generally, in a large graph, vertices are clustered into different communities such that vertices within the same community behave similarly. Such structural property is captured by the stochastic block model (SBM), where each vertex is assigned to a block and the probability that an edge exists between two vertices depends only on their respective block memberships.
\item Formally, the SBM is determined by the number of blocks K (generally way less than the number of vertices N), block proportion vector ?, and block probability matrix B.
\item Now if we consider the SBM as a random dot product graph, all vertices in the same block would have identical latent positions.
\end{itemize}

\subsection{Estimator $\hat{P}$ Based on Adjacency Spectral Embedding}
\begin{itemize}
\item In order to take advantage of the underlying low dimensions of the RDPG, we would like to use the adjacency spectral embedding (ASE) studied by Sussman et. al. to enforce a low rank approximation on the adjacency matrix A, which will decrease the variance if we embed it into the right dimension.
\item Due to the underlying block-distibuted RDPG structure of graphs, enforcing this low rank approximation on $\bar{A}$ will provide a better estimate for the true mean matrix $P$.
\end{itemize}

\subsection{Performance Evaluation: Relative Efficiency}
\begin{itemize}
\item To compare the performance between $\hat{P}$ and $\bar{A}$, we examine the relative efficiency (RE), in mean squared error (MSE), among the two defined as: $RE_{ij} = \frac{MSE(\hat{P}_{ij})}{MSE(\bar{A}_{ij})}$.
\end{itemize}


\section{Results}

\subsection{Theoretical Results}
\begin{theorem}
\label{thm:ARE}
For any $i$ and $j$, conditioning on $X_i = \nu_{\tau_i}$ and $X_j = \nu_{\tau_j}$, we have
\[
	\mathrm{ARE}(\bar{A}_{ij}, \hat{P}_{ij}) = 0.
\]
And for $N$ large enough, conditioning on $X_i = \nu_{\tau_i}$ and $X_j = \nu_{\tau_j}$, we have
\[
	\mathrm{RE}(\bar{A}_{ij}, \hat{P}_{ij}) \approx
    \frac{1/\rho_{\tau_i} + 1/\rho_{\tau_j}}{N}.
\]
\end{theorem}

\begin{lemma}
\label{lm:VarPhat}
In the same setting as above, for any $i, j$, conditioning on $X_i = \nu_{\tau_i}$ and $X_j = \nu_{\tau_j}$, we have
\[
	\lim_{n \to \infty} N \cdot \mathrm{Var}(\hat{P}_{ij}) =
    \frac{1/\rho_{\tau_i} + 1/\rho_{\tau_j}}{M} P_{ij} (1 - P_{ij}).
\]
And for $N$ large enough, conditioning on $X_i = \nu_{\tau_i}$ and $X_j = \nu_{\tau_j}$, we have
\[
	E[(\hat{P}_{ij} - P_{ij})^2] \approx
    \frac{1/\rho_{\tau_i} + 1/\rho_{\tau_j}}{M N} P_{ij}(1-P_{ij}).
\]
\end{lemma}

\subsection{Validation with Simulations}
\begin{itemize}
\item We demonstrate the theoretical results in Section 3.1, the variance of $\hat{P}$ and the relative efficiency, via various Monte Carlo simulation experiments. Specifically, we consider the 2-block SBM parameterized by
\begin{equation*}
B = \begin{bmatrix}
0.42 & 0.2 \\
0.2 & 0.7
\end{bmatrix}
,\qquad \rho = \begin{bmatrix}
0.5 & 0.5
\end{bmatrix}.
\end{equation*}
\item For each Monte Carlo replicate, we generate $M$ random graphs with known vertex correspondence under the SBM described above. And details.
\item Given $M$ graphs, we can calculate $\bar{A}$ and $\hat{P}$ assuming that $d = \mathrm{rank}(B) = 2$ is known. 
\item By checking the averaging MSE and RE of the two estimates $\hat{P}$ and $\bar{A}$ over 1000 Monte Carlo replicates, we demonstrate that the theoretical results in Section 3.1.
\item Figure plots the scaled average RE with different $N$ and fixed $M$ of 1000 Monte Carlo replicates. 
\item To verify Theorem and Lemma hold with different $\rho$, Figure shows the average MSE and average RE with $N = 500$ and $M = 100$ while changing $\rho_1$ from 0.1 to 0.9. 
\end{itemize}

\subsection{CoRR Brain Graphs: Cross-Validation}
\begin{itemize}
\item To demonstrate that the $\hat{P}$ estimate is valid under data that does not perfectly follow a SBM, we examine three datasets, JHU, desikan and CPAC200, which are sets of 454 brain connectomes with different number of nodes generated from fMRI scans available at the Consortium for Reliability and Reproducibility (CoRR).
\item To compare $\bar{A}$ and $\hat{P}$ we perform a cross-validation study to examine the impact of the number of available graphs $M$.
\item Figures demonstrate that our algorithm gives a better estimate $\hat{P}$ according to all three datasets. 
\item When $M$ is small, $\bar{A}$ has large variance which leads to large MSE. Meanwhile, $\hat{P}$ reduces the variance by taking advantages of the graph structure and outperforms $\bar{A}$ dramatically.
\item Moreover, Zhu and Ghodsi's algorithm and USVT algorithm both do a good job for selecting the dimension to embed.
\item Simulation with $P$ being the mean graph of the real data shows that $\hat{P}$ still does a good job when the low rank assumption is violated.
\end{itemize}




\section{Discussion}
\begin{itemize}
\item Scheinerman's method is better than row sum divided by $N-1$ in diagonal augmentation.
\end{itemize}






\section{Methods}

\subsection{Algorithm}
\begin{itemize}
\item Detailed description of the algorithm for our estimator $\hat{P}$.
\end{itemize}

\subsection{Choosing Dimension}
\begin{itemize}
\item Often in dimensionality reduction techniques, the choice for dimension, d, relies on visually analyzing a plot of the ordered eigenvalues, looking for a ``gap'' or ``elbow'' in the scree-plot.
\item USVT is a simple estimation procedure that works for any matrix that has ``a little bit of structure''.
\end{itemize}

\subsection{Graph Diagonal Augmentation}
\begin{itemize}
\item The graphs examined in this work are hollow, in that there are no self-loops and thus the diagonal entries of the adjacency matrix are 0. This leads to a bias in the calculation of the eigenvectors.
\item We minimize this bias by using an iterative method developed by Scheinerman and Tucker.
\end{itemize}

\subsection{Source Code}

\subsection{Dataset Description}
Detailed description of the data we are using.

\subsection{Outline for the Proof of the Theorems}







\end{document}